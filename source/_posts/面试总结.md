---
title: 面试总结
date: 2018-03-29 16:11:49
tags:
---
# 算法面试总结

## 机器学习部分

### 模型评估与选择

#### 评估方法

#### 性能度量

#### 偏差与方差

---

### 线性模型

#### 基本形式

线性模型形式简单、易于建模，但却蕴含着机器学习中的一些重要思想。线性模型试图学得一个通过属性的线性组合来进行预测的函数，即

$$f(x)= \omega_1 x_1 + \omega_2 x_2 + \cdots  + \omega_d x_d + b$$

一般向量形式写成

$$f(x) = \omega^T x + b$$

#### 线性回归

##### 最小二乘法

基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧式距离之和最小（也称残差平方和最小）【残差：实际观测值和预测估计值之差】。其数学表达式为：

$$\min _{\omega} {||X \omega - y||}_2^2$$

求解 $\omega$ 和 b 使 $E_(\omega,b)=\sum_{i=1}^m(y_i - \omega x_i - b)$最小化的过程，称为线性回归模型的最小二乘“参数估计”。

##### 正则化

* L1正则

* L2正则

##### Lasso 回归

Lasso 是估计稀疏系数的线性模型。它是由一个带有L1-norm的正则项的线性模型。最小化目标函数是：

$$\min_\omega \frac{1}{2n_{samples}}||X\omega - y||_2^2 + \alpha ||\omega||_1$$

##### 岭回归（Radge）

岭回归就是带L2系数惩罚的线性模型，

$$\min_\omega||X\omega - y||_2^2 + \alpha||\omega||_2^2$$

#### 逻辑回归（对数几率回归）

##### 广义线性模型

一般地，对于单调可微函数$g(·)$，令

$$y = g^{-1}(\omega x + b)$$

这样得到的模型称为“广义线性模型”，其中函数$g(·)$称为“联系函数”。

##### LR模型原理

从广义线性模型出发，想要做分类任务，只需找到一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。对数几率函数（一种Sigmoid函数）正是这样一个常用的替代函数：

$$y=\frac{1}{1+e^{-z}}$$

即 $y=g^{-1}(·)$。有

$$y = \frac{1}{1+e^{-(\omega^T x + b)}}$$

有：

$$ln \frac{y}{1-y}=\omega^T x + b$$

一个事件的几率（odds）被定义为该事件发生于不发生的概率的比值,对于二分类问题（二项分布）：

$$odds=\frac{p}{1-p}$$，

对数几率：

$$logit(p)=log\frac{p}{1-p}$$。

由此可看出，式$y=\frac{1}{1+e^{(\omega^T x + b)}}$ 实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率，因此，其对应的模型称为“对数几率（逻辑）回归”。

然后，为了确定参数$\omega$和$b$，将逻辑模型中的y看做后验概率估计$p(y=1|x)$，则有：

$$ln\frac{p(y=1|x)}{p(y=0|x)}=\omega^T x + b$$

显然，有

$$p(y=1|x)=\frac{e^{(\omega^T x + b)}}{1 + e^{(\omega^T x + b)}}$$

$$p(y=0|x)=\frac{1}{1 + e^{(\omega^T x + b)}}$$

##### 参数估计

广义线性模型的参数估计常通过加权最小二乘法或极大似然法。

这里使用极大似然法，其思想是找到一组参数，使得在这组参数下，样本属于真实标记的似然度（概率）最大。

$$L(\omega,b)= \prod{[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-{y_i}}}$$

对数似然：

$$lnL(\omega, b)=\sum{[y_iln \pi(x_i)+(1-y_i)ln(1-\pi(x_i))]}$$
$$=\sum{[y_iln \frac{\pi(x_i)}{1-\pi{(x_i)}}+ln(1-\pi(x_i))]}$$
$$=\sum{[y_i(\omega \cdot x_i)-ln(1+e^{\omega \cdot x_i})]}$$

对应的损失函数：

$$J(\omega) = - \frac{1}{N}lnL(\omega)$$

##### 最优化方法

逻辑回归模型的参数估计中，最后就是对J(W)求最小值。这种不带约束条件的最优化问题，常用梯度下降或牛顿法来解决。

使用梯度下降法求解逻辑回归参数估计

求J(w)梯度：g(w)：


---

### 支持向量机（SVM）

#### 原理

#### 间隔与支持向量

#### 线性可分与硬间隔

#### 线性SVM和软间隔

#### 非线性可分SVM与核函数

#### 序列最小最优化算法（SMO）

---

### 决策树

#### 原理

#### ID3、C4.5、CART

#### 剪枝

#### 连续与缺失值

---

### 集成学习（Ensemble）

#### Bagging与随机森林

##### Bagging

##### 随机森林

#### Boosting

##### AdaBoost

##### GBDT

##### GBM

一个简单的GBM二分类伪代码：

1. 初始分类目标的参数值
2. 对所有的分类树进行迭代：
    2.1 根据前一轮分类树的结果更新分类目标的权重值（被错误分类的有更高的权重）
    2.2 用训练的子样本建模
    2.3 用所得模型对所有的样本进行预测
    2.4 再次根据分类结果更新权重值
3. 返回最终结果

##### XGBoost

#### 原理

#### 参数调优

我们会使用和GBM中相似的方法。需要进行如下步骤：

1. 选择较高的学习速率(learning rate)。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。

2. 对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数，待会儿我会举例说明。

3. xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。

4. 降低学习速率，确定理想参数。

**两个重要的思想：**

1. 仅仅靠参数的调整和模型的小幅优化，想要让模型的表现有个大幅度提升是不可能的。GBM的最高得分是0.8487，XGBoost的最高得分是0.8494。确实是有一定的提升，但是没有达到质的飞跃。
2. 要想让模型的表现有一个质的飞跃，需要依靠其他的手段，诸如，特征工程(feature egineering) ，模型组合(ensemble of model),以及堆叠(stacking)等。

##### LightGBM

#### Stacking
