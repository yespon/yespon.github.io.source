---
title: TensorFlow基本概念
date: 2018-03-05 15:07:26
category:
    - 机器学习
tags: 
    - TensorFlow
---
# TensorFlow基本概念

开始使用 TensorFlow 时，我们需要了解下其基本组成元素：

<!--more-->

- 使用**计算图（graph )** 定义计算任务

- 在被称之为**会话 (Session)** 的**上下文 (context)** 中，运行**计算图（graph）**

- 使用**张量（tensor）**表示数据

- 通过**变量（Variable）**维护状态，对应的还有**常量（constant）**

- 使用**注入（feed）**（喂数据）为任意**操作**(arbitrary **operation**) 赋值，使用**取回（fetch）**从任意操作中获取数据

TensorFlow是一个编程系统，结合下图，我们从基本元素开始说起。

![](TensorFlow基本概念\1.gif)

## 1. 张量(Tensor)

名字就是TensorFlow，直观来看，就是张量的流动。张量(tensor)，即任意维度的数据，一维、二维、三维、四维等数据统称为张量。而张量的流动则是指保持计算节点不变，让数据进行流动。这样的设计是针对连接式的机器学习算法，比如逻辑斯底回归，神经网络等。连接式的机器学习算法可以把算法表达成一张图，张量从图中从前到后走一遍就完成了前向运算；而残差从后往前走一遍，就完成了后向传播。

### 1.1 维度 (Shape)

TensorFlow中使用了三种记号描述张量的维度：阶，形状以及维数。下表展示了他们之间的关系：

阶 | 形状 | 维数 | 实例
 -|-|-|-
0 | [ ] | 0-D | 一个0维张量是一个常量，如5
1 | [D0] | 1-D | 一个1维张量是一个矢量，如[5, 4]
2 | [D0, D1] | 2-D | 一个2维张量是一个矩阵，如[[5, 4], [2, 3]]
3 | [D0, D1, D2] | 3-D | 一个3维张量是一个立方矩阵，如[[[5, 4], [2, 3]], [[5, 4], [2, 3]]
n | [D0, D1, D2,….Dn] | n-D | 一个n维张量是一个多维数组

### 1.2 阶（Rank）

在TensorFlow系统中，张量的维数来被描述为阶。但是张量的阶和矩阵的阶并不是同一个概念：张量的阶（关于如顺序、度数或者是n维）是张量维数的一个数量描述。比如，Python中的list列表就是2阶。

你可以认为零阶张量是一个常量；一阶张量是一个向量；二阶张量就是我们平常所说的矩阵，你可以用语句t[i, j]来访问其中的任何元素；对于三阶张量，你可以用t[i, j, k]来访问其中的任何元素。

### 1.3 数据类型 (Type)

除了维度，tensor还有一个数据类型属性。你可以为一个张量指定下列数据类型中的任意一个类型：

![](TensorFlow基本概念\2.png)

## 2. 算子(operation)

节点被称之为op（节点也叫操作、算子，是operation的缩写）。一个op获得0个或多个tensor，执行计算产生0个或多个tensor。

## 3. 边（edge）

TensorFlow，字面意思就是张量的流动（flow）。
TF的图中的边分为两种：

* 正常边，正常边上可以流动数据，即正常边就是tensor。计算图的一条边，就是一个tensor。而张量的流动则是指保持计算节点不变，让数据进行流动。tensor是一个数据类型的一维、二维、三维、四维等多维数组。例如，你可以把一组图像集表示为一个四维浮点数的数组，这四个维度分别是 [batch, height, width, channels]。

* 特殊边，又称作控制依赖，(control dependencies)
  * 没有数据从特殊边上流动，但是特殊边却可以控制节点之间的依赖关系，在特殊边的起始节点完成运算之前，特殊边的结束节点不会被执行。
  * 也不仅仅非得有依赖关系才可以用特殊边，还可以有其他用法，比如为了控制内存的时候，可以让两个实际上并没有前后依赖关系的运算分开执行。
  * 特殊边可以在client端被直接使用。

## 4. 核(kernel)

TF中还有一个概念是kernel，kernel是operation在某种设备上的具体实现。TF的库通过注册机制来定义op和kernel，所以可以通过链接一个其他的库来进行kernel和op的扩展。

## 5. 计算图（graph）

节点和边相互连接成计算图，一个计算图描述了一次计算过程。

这是一个声明式的编程方式，如同做菜，我们需要先把主材和佐料都准备好，才能添油烹制。TensorFlow的计算方式也是如此。在构建阶段，我们需要把网络（如神经网络）以计算图的形式构建出来，接着启动会话（session），运行先前构建的图，得到目标结果。

## 6. 会话（session）

使用TensorFlow编写的程序，通常被组织成一个构建阶段和一个运行阶段：在构建阶段，操作的执行步骤被描述成一个计算图；在运行阶段，使用会话执行计算图中的操作。

为了得到结果，计算图必须在会话里被启动。会话将计算图的op分发到诸如CPU或GPU之类的设备上，同时提供执行op的方法。这些方法执行后，将产生的tensor返回。在Python语言中, 返回的tensor是numpy ndarray对象；在C和C++语言中，返回的tensor是tensorflow Tensor实例。

## 7. 变量（variable）

在运行计算图过程中，变量用于维护某个参数的状态。TensorFlow通常会将一个统计模型中的参数表示为一组变量，例如你可以将一个神经网络的权重作为某个变量存储在一个tensor中。在训练过程中, 通过重复运行计算图，更新这个tensor。

## 参考

[1] [tensorflow原理](http://blog.csdn.net/weixin_30014549/article/details/52529036)

[2] [tensorflow架构](http://blog.csdn.net/stdcoutzyx/article/details/51645396)